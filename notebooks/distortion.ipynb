{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "###############################################################################\n",
    "# 1. CLIP WITH NO GRAD (MIMICS THE JAX custom_jvp APPROACH)\n",
    "###############################################################################\n",
    "class ClipNoGrad(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    A clamp operation whose gradient is a no-op.\n",
    "    That is, grad_output passes through unchanged, so clamp() does not\n",
    "    clip or modify gradients in backprop.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, lower, upper):\n",
    "        # Just store for debugging. We do not actually use them in backward.\n",
    "        ctx.save_for_backward(input, lower, upper)\n",
    "        return torch.clamp(input, lower.item(), upper.item())\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # No gradient effect from the clamp in backward:\n",
    "        return grad_output, None, None\n",
    "\n",
    "\n",
    "def generate_clip_nograd_fn(a_min: float, a_max: float):\n",
    "    \"\"\"\n",
    "    Generates a function that clamps inputs to [a_min, a_max] but\n",
    "    does not modify gradients.\n",
    "    \"\"\"\n",
    "    lower = torch.tensor(a_min)\n",
    "    upper = torch.tensor(a_max)\n",
    "\n",
    "    def clip_nograd(a: torch.Tensor) -> torch.Tensor:\n",
    "        return ClipNoGrad.apply(a, lower, upper)\n",
    "\n",
    "    return clip_nograd\n",
    "\n",
    "\n",
    "# Example bounds for “finite clip”\n",
    "min_val = -1e6\n",
    "max_val =  1e6\n",
    "clip_finite_nograd = generate_clip_nograd_fn(min_val, max_val)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 2. HELPER FUNCTIONS\n",
    "###############################################################################\n",
    "tiny_val = 1e-12\n",
    "\n",
    "def remove_zero(x: torch.Tensor, eps: float = 1e-12) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    If |x| is below eps, replace it with +eps (assumes x is either 0 or small).\n",
    "    Helps avoid division by zero or NaNs.\n",
    "    \"\"\"\n",
    "    return torch.where(torch.abs(x) < eps, x.sign() * eps, x)\n",
    "\n",
    "\n",
    "def safe_sign(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Returns sign(x) in [-1, 0, 1]. By default, torch.sign(0) = 0,\n",
    "    which is fine if you want sign(0) = 0.\n",
    "    \"\"\"\n",
    "    return torch.sign(x)\n",
    "\n",
    "\n",
    "def safe_expm1(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    A safer expm1 if you want to clamp x to avoid overflow.\n",
    "    expm1(x) = e^x - 1\n",
    "    \"\"\"\n",
    "    # Adjust clamp range as needed\n",
    "    return torch.expm1(torch.clamp(x, min=-20.0, max=20.0))\n",
    "\n",
    "\n",
    "def safe_log1p(x: torch.Tensor, eps: float = 1e-12) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    A safer log1p if x can be zero or small. log1p(x) = log(1 + x).\n",
    "    Here we assume x >= 0 in typical usage (e.g. x=|something|).\n",
    "    \"\"\"\n",
    "    return torch.log1p(x + eps)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 3. POWER LADDER (Tukey's Ladder of Powers)\n",
    "###############################################################################\n",
    "def power_ladder(\n",
    "    x: torch.Tensor,\n",
    "    p: torch.Tensor,\n",
    "    premult: float = None,\n",
    "    postmult: float = None\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Tukey's power ladder with a +1 inside, plus some scaling and special cases:\n",
    "      y = sign(x) * (|p - 1| / p) * [((|x| / |p-1|) + 1)^p - 1]\n",
    "\n",
    "    Special cases: if p = 1, or p = 0, or ±∞, we do a different expression:\n",
    "      - p = 1 => just |x|\n",
    "      - p = 0 => log1p(|x|)\n",
    "      - p = -inf => -expm1(-|x|)\n",
    "      - p =  inf =>  expm1(|x|)\n",
    "    \"\"\"\n",
    "    if premult is not None:\n",
    "        x = x * premult\n",
    "\n",
    "    xp = torch.abs(x)  # |x|\n",
    "\n",
    "    # Denominator for the normal case\n",
    "    abs_p_minus_1 = torch.abs(p - 1)\n",
    "    denom = torch.maximum(abs_p_minus_1, torch.tensor(tiny_val, device=x.device, dtype=x.dtype))\n",
    "    xs = xp / denom\n",
    "\n",
    "    # Ensure p isn't zero or extremely small in the main formula\n",
    "    p_removed_zero = remove_zero(p, eps=tiny_val)\n",
    "\n",
    "    # Clip p to finite range (no grad effect)\n",
    "    p_safe = clip_finite_nograd(p_removed_zero)\n",
    "\n",
    "    # The \"default\" ladder expression:\n",
    "    ladder_expr = torch.abs(p_safe - 1.0) / p_safe * (((xs + 1.0) ** p_safe) - 1.0)\n",
    "    ladder_expr = clip_finite_nograd(ladder_expr)  # also clamp the result\n",
    "\n",
    "    # Decide which expression to use, based on p\n",
    "    # If p is a single scalar (not a tensor), we can do Python if-elif.\n",
    "    # If p is a tensor that can vary element-wise, do torch.where() checks\n",
    "    sign_x = safe_sign(x)\n",
    "\n",
    "    # We'll handle the \"float scalar\" case first.\n",
    "    if p.numel() == 1:\n",
    "        # p is a single scalar, we can do normal Python comparisons\n",
    "        p_val = p.item()\n",
    "        if p_val == 1.0:\n",
    "            core = xp\n",
    "        elif p_val == 0.0:\n",
    "            core = safe_log1p(xp)\n",
    "        elif p_val == float('-inf'):\n",
    "            # -∞ => -expm1(-|x|)\n",
    "            core = -safe_expm1(-xp)\n",
    "        elif p_val == float('inf'):\n",
    "            core = safe_expm1(xp)\n",
    "        else:\n",
    "            core = ladder_expr\n",
    "    else:\n",
    "        # p is a tensor, do elementwise checks. (Exact floating comparison is tricky,\n",
    "        # but we'll assume you literally set some entries to float('inf') or 1.0, etc.)\n",
    "        eq1 = (p == 1.0)\n",
    "        eq0 = (p == 0.0)\n",
    "        eq_neg_inf = (p == float('-inf'))\n",
    "        eq_pos_inf = (p == float('inf'))\n",
    "\n",
    "        core = ladder_expr\n",
    "        core = torch.where(eq1, xp, core)\n",
    "        core = torch.where(eq0, safe_log1p(xp), core)\n",
    "        core = torch.where(eq_neg_inf, -safe_expm1(-xp), core)\n",
    "        core = torch.where(eq_pos_inf, safe_expm1(xp), core)\n",
    "\n",
    "    y = sign_x * core\n",
    "\n",
    "    if postmult is not None:\n",
    "        y = y * postmult\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 4. DISTORTION LOSS\n",
    "###############################################################################\n",
    "def lossfun_distortion(t: torch.Tensor, w: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute the pairwise distortion:\n",
    "      iint w[i] w[j] * |t[i] - t[j]| di dj\n",
    "    which breaks down into an \"inter\" term (between different intervals)\n",
    "    and an \"intra\" term (within the same interval).\n",
    "    Args:\n",
    "      t: shape [..., N+1]\n",
    "      w: shape [..., N], the weights in each interval\n",
    "    Returns:\n",
    "      A tensor of shape [...], the distortion.\n",
    "    \"\"\"\n",
    "    # Midpoints of each interval\n",
    "    ut = 0.5 * (t[..., 1:] + t[..., :-1])  # shape [..., N]\n",
    "\n",
    "    # Pairwise distance between midpoints\n",
    "    # Expand ut so that we get broadcast shapes [..., N, 1] vs [..., 1, N]\n",
    "    dut = torch.abs(ut[..., :, None] - ut[..., None, :])  # [..., N, N]\n",
    "\n",
    "    # \"Inter\" term: sum over i w[i] * sum over j of w[j]*|t_i - t_j|\n",
    "    # shape [..., N]\n",
    "    inter_term = torch.sum(w * torch.sum(w[..., None, :] * dut, dim=-1), dim=-1)\n",
    "\n",
    "    # \"Intra\" term: sum of w[i]^2 * (t[i+1] - t[i]) / 3\n",
    "    dt = t[..., 1:] - t[..., :-1]  # shape [..., N]\n",
    "    intra_term = torch.sum((w**2) * dt, dim=-1) / 3.0\n",
    "\n",
    "    return inter_term + intra_term\n",
    "\n",
    "# --------------------\n",
    "# First, convert the original numpy-based functions to torch versions\n",
    "# so we can do automatic differentiation in PyTorch.\n",
    "# --------------------\n",
    "\n",
    "def moment_0_int_torch(a, b, s):\n",
    "    # a, b, s are torch Tensors\n",
    "    return (1.0 - torch.exp(s * (a - b))) / s\n",
    "\n",
    "def moment_1_int_torch(a, b, s):\n",
    "    return (a*s - (b*s + 1.0)*torch.exp(s*(a - b)) + 1.0) / (s**2)\n",
    "\n",
    "def self_dist_torch(a, b, s):\n",
    "    ds = s*(a - b)\n",
    "    return (2.0*s*(a-b)*torch.exp(ds) + 1.0 - torch.exp(2.0*ds)) / (s**3)\n",
    "\n",
    "def update_distortion_state_torch(state, t1, t2, sigma):\n",
    "    \"\"\"\n",
    "    state is [x, y, z, w, v, T]\n",
    "    t1, t2, sigma are scalars (torch tensors)\n",
    "    \"\"\"\n",
    "    x, y, z, w, v, T = state\n",
    "\n",
    "    m0 = T * moment_0_int_torch(t1, t2, sigma)\n",
    "    m1 = T * moment_1_int_torch(t1, t2, sigma)\n",
    "\n",
    "    new_z = z + m0\n",
    "    new_w = w + m1\n",
    "    new_x = x + z*m1\n",
    "    new_y = y + w*m0\n",
    "\n",
    "    # self_dist doesn't get multiplied by state-dependent terms (except T^2),\n",
    "    # so that is direct from the integral:\n",
    "    new_v = v + T*T* self_dist_torch(t1, t2, sigma)\n",
    "\n",
    "    alpha = 1.0 - torch.exp(-sigma * (t2 - t1))\n",
    "    new_T = T * (1.0 - alpha)\n",
    "\n",
    "    return torch.stack([new_x, new_y, new_z, new_w, new_v, new_T])\n",
    "\n",
    "def incremental_distortion_torch(t, sigma):\n",
    "    \"\"\"\n",
    "    t: sorted boundaries, shape [N+1]\n",
    "    sigma: densities, shape [N]\n",
    "    \"\"\"\n",
    "    # state = [x, y, z, w, v, T], initialize as zeros but T=1\n",
    "    state = torch.zeros(6, dtype=torch.float32, device=t.device)\n",
    "    state[-1] = 1.0  # T=1\n",
    "    for i in range(len(sigma)):\n",
    "        state = update_distortion_state_torch(\n",
    "            state,\n",
    "            t[i], t[i+1],\n",
    "            sigma[i]\n",
    "        )\n",
    "    x, y, _, _, v, _ = state\n",
    "    return 2.0*(x - y) + v\n",
    "\n",
    "\n",
    "# --------------------\n",
    "# DEMO: train sigma to minimize incremental_distortion\n",
    "# --------------------\n",
    "# Fix the random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "N = 10  # number of intervals\n",
    "# t: sorted boundaries (randomly generated)\n",
    "ft_np = np.sort(10.0 * np.random.rand(N+1))\n",
    "# Convert t to a torch tensor\n",
    "t_torch = torch.tensor(ft_np, dtype=torch.float32)\n",
    "\n",
    "# Initialize sigma randomly\n",
    "sigma_init = torch.rand(N, dtype=torch.float32)  # shape [N]\n",
    "sigma = torch.nn.Parameter(sigma_init.clone())   # make it a trainable Param\n",
    "\n",
    "# Set up an optimizer (e.g., Adam)\n",
    "optimizer = torch.optim.Adam([sigma], lr=0.1)\n",
    "\n",
    "n_iters = 2000\n",
    "for step in range(n_iters):\n",
    "    optimizer.zero_grad()\n",
    "    loss = incremental_distortion_torch(t_torch, sigma)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Optionally enforce sigma >= 0 if you need a nonnegative density\n",
    "    with torch.no_grad():\n",
    "        sigma.clamp_(min=0.0)\n",
    "\n",
    "    if (step+1) % 200 == 0 or step == 0:\n",
    "        print(f\"Step {step+1:4d}: distortion = {loss.item():.6f}\")\n",
    "\n",
    "print(\"\\nFinal sigma values:\")\n",
    "print(sigma.detach().cpu().numpy())\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
